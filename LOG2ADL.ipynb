{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log2Csv Converter\n",
    "ATRで取得したデータをADLtaggerで扱えるように形式変換する. 処理できるATRのセンサで取得したデータは日分のみ(timestampで日にちを確定できない)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [次でバックして!!!] timestampの小数点以下の0が桁落ち ==> str.zfill(3) + .astype(str)を使え!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil # shutil.rmtree(path)でdirectory tree全体を削除(空でなくても)\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import csv\n",
    "\n",
    "\n",
    "class Log2ADL(object):\n",
    "    \"\"\"\n",
    "    Convert log files which generated by ATR acceleration sensor into ADL tagger format\n",
    "    \"\"\"\n",
    "    def __init__(self, date_str, t_shift, path_to_log_dir='./log/', path_to_output_dir='./ADL/'):\n",
    "        \"\"\"\n",
    "        Argument\n",
    "        --------\n",
    "        date_str: str, the date which logs were recorded. (ex. '2017-09-21')\n",
    "        t_shift: int, to adjust timestump between PC and ATR sensors.[ms]\n",
    "        path_log_dir/path_output_dir: Path to the log/output file directory\n",
    "        \"\"\"\n",
    "        self.path_to_log_dir = path_to_log_dir\n",
    "        self.path_to_output_dir = path_to_output_dir\n",
    "        # タイムスタンプを合わせるための基準時刻を生成\n",
    "        self.base_timestamp = dt.datetime.strptime(date_str, '%Y-%m-%d') + dt.timedelta(milliseconds=t_shift)\n",
    "        print(\"Class was successfully generated [base_timestump=\",self.base_timestamp,\"]\")\n",
    "    \n",
    "    \n",
    "    \"\"\"Read Log Files\"\"\"\n",
    "    def read_logs(self):\n",
    "        # Step.1: 処理するLogファイルの一覧を取得\n",
    "        print(\"Step1: Get log files.\")\n",
    "        log_list = os.listdir(self.path_to_log_dir)\n",
    "        print(\">> Success: \",log_list, \"\\n\")\n",
    "\n",
    "        # Step.2: logファイルをデータフレームに変換\n",
    "        print(\"Step2: Convert log files to pd.DataFrame.\")\n",
    "        df = []\n",
    "        for file_name in log_list:\n",
    "            if file_name.find('.log') > 0:\n",
    "                path_to_file = self.path_to_log_dir + file_name\n",
    "                with open(path_to_file, 'r') as f:\n",
    "                    reader, x = csv.reader(f), []\n",
    "                    for row in reader:\n",
    "                        if \"ags\" in row: x.append(row)\n",
    "                    print(\">> Read CSV [\"+ path_to_file + ']  ==> Sucess(', len(x), \"rows)\")\n",
    "                    df = df + x\n",
    "        self.df = pd.DataFrame(df, columns=[\"sensor\", \"time_ATR\", \"accX\", \"accY\", \"accZ\", \"gyroX\", \"gyroY\", \"gyroZ\"])\n",
    "        print(\">> Success: df.shape=\", self.df.shape, \"\\n\")\n",
    "        return self.df\n",
    "    \n",
    "    \n",
    "    \"\"\"Add Timestamps\"\"\"\n",
    "    def generate_timestamp(self,time):\n",
    "        \"\"\"Convert an ATR timestamp into a datetime object\"\"\"\n",
    "        # Params >> time(integer or str)\n",
    "        # Return >> datetime.datatime\n",
    "        \n",
    "        # Convert milliseconds to r60\n",
    "        time = int(time)\n",
    "        milliseconds, time = time%1000, int(time/1000)\n",
    "        seconds,      time = time%60,   int(time/60)\n",
    "        minutes,      time = time%60,   int(time/60)\n",
    "        hours,        time = time%60,   int(time/60)\n",
    "        # Error Check\n",
    "        if time > 1:\n",
    "            print(\">> Error: timestamp of ATR sensor is invaild format.\")\n",
    "        # 基準時間と合わせる\n",
    "        new_time = self.base_timestamp + dt.timedelta(milliseconds=milliseconds, seconds=seconds, minutes=minutes, hours=hours)\n",
    "        return new_time\n",
    "\n",
    "    def add_timestamps(self):\n",
    "        print(\"Step3: Add Timestamps.\")\n",
    "        df = self.df.sort_values(by=[\"time_ATR\"], ascending=True).reset_index(drop=True)\n",
    "        df[\"timestamp\"] = df[\"time_ATR\"].apply(self.generate_timestamp)\n",
    "        df[\"time\"], df[\"time_milli\"] = df[\"timestamp\"].dt.strftime('%Y%m%d_%H:%M:%S.'), df[\"timestamp\"].dt.microsecond // 1000\n",
    "        df[\"time\"] = df[\"time\"].astype(str) + df[\"time_milli\"].astype(str).str.zfill(3)\n",
    "        #df[\"time\"] = df[\"timestamp\"].apply(lambda x: x.strftime('%Y%m%d_%H:%M:%S.') + \"%03d\" % (x.microsecond // 1000))\n",
    "        df[\"group\"] = df[\"timestamp\"].dt.strftime('%Y%m%d_%H%M')\n",
    "        self.df = df\n",
    "        print(\">> Success: df.shape=\", self.df.shape, \"\\n\")\n",
    "        return df\n",
    "\n",
    "    \n",
    "    \"\"\"Write output\"\"\"\n",
    "    def activate_dir(self, target_path, dir_name):\n",
    "        if not os.path.isdir(target_path+dir_name):\n",
    "            # 存在しない場合はディレクトリを作成\n",
    "            os.mkdir(target_path+dir_name)\n",
    "            if os.path.isdir(target_path+dir_name):\n",
    "                print(\">> Directory was created [\"+ target_path+dir_name +\"]\")\n",
    "        return target_path  + dir_name + '/'\n",
    "\n",
    "    def to_csvs(self):\n",
    "        print(\"Step4: Write CSVs.\")\n",
    "        groups = self.df[\"group\"].drop_duplicates().reset_index(drop=True)\n",
    "        df = self.df\n",
    "        # Outディレクトリをクリア\n",
    "        if os.path.isdir(self.path_to_output_dir):\n",
    "            shutil.rmtree(self.path_to_output_dir)\n",
    "        os.mkdir(self.path_to_output_dir)\n",
    "        print(\">> Clean output directory.\")\n",
    "        for group in groups:\n",
    "            #　書き込む行を選択\n",
    "            df_selected = df[df[\"group\"] == group].sort_values(by=[\"timestamp\"])\n",
    "            # 書き込むディレクトリを選択: Acc\n",
    "            ## ディレクトリの確認\n",
    "            target_path = self.activate_dir(self.path_to_output_dir, \"acc2\")\n",
    "            target_path = self.activate_dir(target_path, \"acc2_R\")\n",
    "            ## 書き込みファイルを指定\n",
    "            target_file_name = group+\"00_acc2.csv\"\n",
    "            # CSV書き込み\n",
    "            df_selected[[\"time\", \"accX\", \"accY\", \"accZ\"]].to_csv(target_path+'/'+target_file_name, index=False, header=[\"time\", \"x\", \"y\", \"z\"])\n",
    "            print(\">> write\", target_path+target_file_name)\n",
    "\n",
    "            # 書き込むディレクトリを選択: Gyro\n",
    "            ## ディレクトリの確認\n",
    "            target_path = self.activate_dir(self.path_to_output_dir, \"Gyro\")\n",
    "            target_path = self.activate_dir(target_path, \"gyro\")\n",
    "            ## 書き込みファイルを指定\n",
    "            target_file_name = group+\"00_gyro.csv\"\n",
    "            # CSV書き込み\n",
    "            df_selected[[\"time\", \"accX\", \"accY\", \"accZ\"]].to_csv(target_path+'/'+target_file_name, index=False, header=[\"time\", \"x\", \"y\", \"z\"])\n",
    "            print(\">> write\", target_path+target_file_name)\n",
    "        print(\">> Success:\", len(groups), \"files were created.\\n\")\n",
    "        \n",
    "        \n",
    "# converter = Log2ADL('2017-09-21', 100)\n",
    "# df_foo = converter.read_logs()\n",
    "# df_foo = converter.add_timestamps()\n",
    "# df_foo = converter.to_csvs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_ms: 683450.0\n",
      "Class was successfully generated [base_timestump= 2018-01-31 00:11:23.450000 ]\n",
      "Step1: Get log files.\n",
      ">> Success:  ['32_torisuke.log'] \n",
      "\n",
      "Step2: Convert log files to pd.DataFrame.\n",
      ">> Read CSV [./data/log/32_torisuke.log]  ==> Sucess( 595970 rows)\n",
      ">> Success: df.shape= (595970, 8) \n",
      "\n",
      "Step3: Add Timestamps.\n",
      ">> Success: df.shape= (595970, 12) \n",
      "\n",
      "Step4: Write CSVs.\n",
      ">> Clean output directory.\n",
      ">> Directory was created [./data/ADL/acc2]\n",
      ">> Directory was created [./data/ADL/acc2/acc2_R]\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_212800_acc2.csv\n",
      ">> Directory was created [./data/ADL/Gyro]\n",
      ">> Directory was created [./data/ADL/Gyro/gyro]\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_212800_gyro.csv\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_212900_acc2.csv\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_212900_gyro.csv\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_213000_acc2.csv\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_213000_gyro.csv\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_213100_acc2.csv\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_213100_gyro.csv\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_213200_acc2.csv\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_213200_gyro.csv\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_213300_acc2.csv\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_213300_gyro.csv\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_213400_acc2.csv\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_213400_gyro.csv\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_213500_acc2.csv\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_213500_gyro.csv\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_213600_acc2.csv\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_213600_gyro.csv\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_213700_acc2.csv\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_213700_gyro.csv\n",
      ">> write ./data/ADL/acc2/acc2_R/20180131_213800_acc2.csv\n",
      ">> write ./data/ADL/Gyro/gyro/20180131_213800_gyro.csv\n",
      ">> Success: 11 files were created.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor</th>\n",
       "      <th>time_ATR</th>\n",
       "      <th>accX</th>\n",
       "      <th>accY</th>\n",
       "      <th>accZ</th>\n",
       "      <th>gyroX</th>\n",
       "      <th>gyroY</th>\n",
       "      <th>gyroZ</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time</th>\n",
       "      <th>time_milli</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ags</td>\n",
       "      <td>76649091</td>\n",
       "      <td>658</td>\n",
       "      <td>1954</td>\n",
       "      <td>10022</td>\n",
       "      <td>213</td>\n",
       "      <td>-720</td>\n",
       "      <td>87</td>\n",
       "      <td>2018-01-31 21:28:52.541</td>\n",
       "      <td>20180131_21:28:52.541</td>\n",
       "      <td>541</td>\n",
       "      <td>20180131_2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ags</td>\n",
       "      <td>76649092</td>\n",
       "      <td>665</td>\n",
       "      <td>1945</td>\n",
       "      <td>9996</td>\n",
       "      <td>220</td>\n",
       "      <td>-704</td>\n",
       "      <td>78</td>\n",
       "      <td>2018-01-31 21:28:52.542</td>\n",
       "      <td>20180131_21:28:52.542</td>\n",
       "      <td>542</td>\n",
       "      <td>20180131_2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ags</td>\n",
       "      <td>76649093</td>\n",
       "      <td>675</td>\n",
       "      <td>2001</td>\n",
       "      <td>10018</td>\n",
       "      <td>214</td>\n",
       "      <td>-682</td>\n",
       "      <td>78</td>\n",
       "      <td>2018-01-31 21:28:52.543</td>\n",
       "      <td>20180131_21:28:52.543</td>\n",
       "      <td>543</td>\n",
       "      <td>20180131_2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ags</td>\n",
       "      <td>76649094</td>\n",
       "      <td>680</td>\n",
       "      <td>2025</td>\n",
       "      <td>9991</td>\n",
       "      <td>214</td>\n",
       "      <td>-686</td>\n",
       "      <td>64</td>\n",
       "      <td>2018-01-31 21:28:52.544</td>\n",
       "      <td>20180131_21:28:52.544</td>\n",
       "      <td>544</td>\n",
       "      <td>20180131_2128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ags</td>\n",
       "      <td>76649095</td>\n",
       "      <td>665</td>\n",
       "      <td>2006</td>\n",
       "      <td>9961</td>\n",
       "      <td>199</td>\n",
       "      <td>-701</td>\n",
       "      <td>75</td>\n",
       "      <td>2018-01-31 21:28:52.545</td>\n",
       "      <td>20180131_21:28:52.545</td>\n",
       "      <td>545</td>\n",
       "      <td>20180131_2128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sensor  time_ATR accX  accY   accZ gyroX gyroY gyroZ  \\\n",
       "0    ags  76649091  658  1954  10022   213  -720    87   \n",
       "1    ags  76649092  665  1945   9996   220  -704    78   \n",
       "2    ags  76649093  675  2001  10018   214  -682    78   \n",
       "3    ags  76649094  680  2025   9991   214  -686    64   \n",
       "4    ags  76649095  665  2006   9961   199  -701    75   \n",
       "\n",
       "                timestamp                   time  time_milli          group  \n",
       "0 2018-01-31 21:28:52.541  20180131_21:28:52.541         541  20180131_2128  \n",
       "1 2018-01-31 21:28:52.542  20180131_21:28:52.542         542  20180131_2128  \n",
       "2 2018-01-31 21:28:52.543  20180131_21:28:52.543         543  20180131_2128  \n",
       "3 2018-01-31 21:28:52.544  20180131_21:28:52.544         544  20180131_2128  \n",
       "4 2018-01-31 21:28:52.545  20180131_21:28:52.545         545  20180131_2128  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main 関数\n",
    "\"\"\"\n",
    "ts_minutes = 10\n",
    "ts_seconds = ts_minutes*60 + 88 - 5 + 0.45\n",
    "ts_ms      = ts_seconds*1000\n",
    "print(\"ts_ms:\",  ts_ms)\n",
    "\n",
    "converter = Log2ADL('2018-01-31', ts_ms,  path_to_log_dir='./data/log/', path_to_output_dir='./data/ADL/')\n",
    "df_foo = converter.read_logs()\n",
    "\n",
    "#converter.df = converter.df[:50000]\n",
    "\n",
    "df_foo = converter.add_timestamps()\n",
    "converter.to_csvs()\n",
    "display(df_foo.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ts_ms: 0\n",
      "Class was successfully generated [base_timestump= 2018-02-04 00:00:00 ]\n",
      "Step1: Get log files.\n",
      ">> Success:  ['45_maekawa.log'] \n",
      "\n",
      "Step2: Convert log files to pd.DataFrame.\n",
      ">> Read CSV [/root/upconversion/data/2018_01_16/45_maekawa/log/45_maekawa.log]  ==> Sucess( 714080 rows)\n",
      ">> Success: df.shape= (714080, 8) \n",
      "\n",
      "Step3: Add Timestamps.\n",
      ">> Success: df.shape= (714080, 12) \n",
      "\n",
      "Step4: Write CSVs.\n",
      ">> Clean output directory.\n",
      ">> Directory was created [/root/upconversion/data/2018_01_16/45_maekawa/data/acc2]\n",
      ">> Directory was created [/root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R]\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_121300_acc2.csv\n",
      ">> Directory was created [/root/upconversion/data/2018_01_16/45_maekawa/data/Gyro]\n",
      ">> Directory was created [/root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro]\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_121300_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_121400_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_121400_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_121500_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_121500_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_121600_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_121600_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_121700_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_121700_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_121800_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_121800_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_121900_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_121900_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_122000_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_122000_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_122100_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_122100_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_122200_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_122200_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_122300_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_122300_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_122400_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_122400_gyro.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/acc2/acc2_R/20180204_122500_acc2.csv\n",
      ">> write /root/upconversion/data/2018_01_16/45_maekawa/data/Gyro/gyro/20180204_122500_gyro.csv\n",
      ">> Success: 13 files were created.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sensor</th>\n",
       "      <th>time_ATR</th>\n",
       "      <th>accX</th>\n",
       "      <th>accY</th>\n",
       "      <th>accZ</th>\n",
       "      <th>gyroX</th>\n",
       "      <th>gyroY</th>\n",
       "      <th>gyroZ</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>time</th>\n",
       "      <th>time_milli</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ags</td>\n",
       "      <td>43992341</td>\n",
       "      <td>41</td>\n",
       "      <td>121</td>\n",
       "      <td>9971</td>\n",
       "      <td>-22</td>\n",
       "      <td>-146</td>\n",
       "      <td>182</td>\n",
       "      <td>2018-02-04 12:13:12.341</td>\n",
       "      <td>20180204_12:13:12.341</td>\n",
       "      <td>341</td>\n",
       "      <td>20180204_1213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ags</td>\n",
       "      <td>43992342</td>\n",
       "      <td>70</td>\n",
       "      <td>128</td>\n",
       "      <td>9944</td>\n",
       "      <td>-52</td>\n",
       "      <td>-150</td>\n",
       "      <td>168</td>\n",
       "      <td>2018-02-04 12:13:12.342</td>\n",
       "      <td>20180204_12:13:12.342</td>\n",
       "      <td>342</td>\n",
       "      <td>20180204_1213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ags</td>\n",
       "      <td>43992343</td>\n",
       "      <td>75</td>\n",
       "      <td>121</td>\n",
       "      <td>9986</td>\n",
       "      <td>-72</td>\n",
       "      <td>-129</td>\n",
       "      <td>168</td>\n",
       "      <td>2018-02-04 12:13:12.343</td>\n",
       "      <td>20180204_12:13:12.343</td>\n",
       "      <td>343</td>\n",
       "      <td>20180204_1213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ags</td>\n",
       "      <td>43992344</td>\n",
       "      <td>70</td>\n",
       "      <td>123</td>\n",
       "      <td>9983</td>\n",
       "      <td>-60</td>\n",
       "      <td>-123</td>\n",
       "      <td>174</td>\n",
       "      <td>2018-02-04 12:13:12.344</td>\n",
       "      <td>20180204_12:13:12.344</td>\n",
       "      <td>344</td>\n",
       "      <td>20180204_1213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ags</td>\n",
       "      <td>43992345</td>\n",
       "      <td>58</td>\n",
       "      <td>153</td>\n",
       "      <td>9915</td>\n",
       "      <td>-46</td>\n",
       "      <td>-143</td>\n",
       "      <td>164</td>\n",
       "      <td>2018-02-04 12:13:12.345</td>\n",
       "      <td>20180204_12:13:12.345</td>\n",
       "      <td>345</td>\n",
       "      <td>20180204_1213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sensor  time_ATR accX accY  accZ gyroX gyroY gyroZ               timestamp  \\\n",
       "0    ags  43992341   41  121  9971   -22  -146   182 2018-02-04 12:13:12.341   \n",
       "1    ags  43992342   70  128  9944   -52  -150   168 2018-02-04 12:13:12.342   \n",
       "2    ags  43992343   75  121  9986   -72  -129   168 2018-02-04 12:13:12.343   \n",
       "3    ags  43992344   70  123  9983   -60  -123   174 2018-02-04 12:13:12.344   \n",
       "4    ags  43992345   58  153  9915   -46  -143   164 2018-02-04 12:13:12.345   \n",
       "\n",
       "                    time  time_milli          group  \n",
       "0  20180204_12:13:12.341         341  20180204_1213  \n",
       "1  20180204_12:13:12.342         342  20180204_1213  \n",
       "2  20180204_12:13:12.343         343  20180204_1213  \n",
       "3  20180204_12:13:12.344         344  20180204_1213  \n",
       "4  20180204_12:13:12.345         345  20180204_1213  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Main 関数: 2018.02.01\n",
    "\"\"\"\n",
    "ts_minutes = 0\n",
    "ts_seconds = ts_minutes*60\n",
    "ts_ms      = ts_seconds*1000\n",
    "print(\"ts_ms:\",  ts_ms)\n",
    "\n",
    "sub_name = \"45_maekawa\"\n",
    "path_to_log_dir = \"/root/upconversion/data/2018_01_16/{}/log/\".format(sub_name)\n",
    "path_to_output_dir = \"/root/upconversion/data/2018_01_16/{}/data/\".format(sub_name)\n",
    "\n",
    "converter = Log2ADL('2018-02-04', ts_ms,  path_to_log_dir=path_to_log_dir, path_to_output_dir=path_to_output_dir)\n",
    "df_foo = converter.read_logs()\n",
    "\n",
    "#converter.df = converter.df[:50000]\n",
    "\n",
    "df_foo = converter.add_timestamps()\n",
    "converter.to_csvs()\n",
    "display(df_foo.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
